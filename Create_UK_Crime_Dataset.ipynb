{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "victorian-discrimination",
   "metadata": {},
   "source": [
    "# Mining Association Rules in Distibuted Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-pulse",
   "metadata": {},
   "source": [
    "Matheus Schmitz  \n",
    "<a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a>  \n",
    "<a href=\"https://matheus-schmitz.github.io/\">Github Portfolio</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-trade",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-lafayette",
   "metadata": {},
   "source": [
    "My intent for this project is to use Spark (this time in combination with R) to mine association rules using data mining techniques. This of couse requires tools suited for Big Data approaches, hence Spark is chosen as it allows for the manipulation of large datasets distributed across multiple computing nodes.\n",
    "\n",
    "Big data presents certain hinderances to neural networks and other gradient descent learning approaches, while being more friendly (less unfriendly?) to techniques that are more easily parallelized, such as those employed when mining association rules. Hence, very commonly such approaches are used to analyse very large datasets, the practice of which is my goal here. Among the multiple algolrithms available, I'll focus on one which is widely regarded as being among the best: the Frequent Pattern Growth Algorithm.\n",
    "\n",
    "For this project I'll be using crime data available from the UK Police's Open Data Portal, which contains a variety of records on all registered crimes. The data is available from 2014 onwards, although I've chosen to work with two years of data, from january 2019 to december 2020, which allow for a control and a test group for exploring the impacts of covid-19 on crime patterns.\n",
    "\n",
    "**Data Source:** https://data.police.uk/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-victor",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worldwide-commissioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files found: 1061\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Before starting the work in R, I need to run a python script to concatenate all CSVs\n",
    "dfs = glob.glob('**/*.csv', recursive=True)\n",
    "print(f'Number of files found: {len(dfs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-generic",
   "metadata": {},
   "source": [
    "There are 44 or 45 CSVs files per month (one per region), considering 24 months the expected number of files was between 1056 and 1080, so seems like we got them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parallel-participant",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1061/1061 [02:18<00:00,  7.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12880086, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now concatete all CSVs into a single datafram\n",
    "result = pd.concat([pd.read_csv(df) for df in tqdm(dfs)], ignore_index=True)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "improved-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then save the dataframe to disk\n",
    "result.to_csv('data/UK_Crime.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preliminary-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the base dataset turned out to be so massive, I'll also create a downsampled size to ensure I can run it\n",
    "result_donwsampled = result.sample(frac=0.001)\n",
    "result_donwsampled.to_csv('data/UK_Crime_downsampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-polish",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/59552212/choosing-support-and-confidence-values-with-ml-fpgrowth-in-sparklyr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
